\chapter{Implementation of machine learning interatomic potentials}
%\section{Modular design of MLIP}

Interatomic potential have been long time used to approximate the potential energy in classical molecular dynamics simulation by decomposing the energy into many-body contributions
%In the picture of the Born-Oppenheimer approximation we can decouple the time propagation of the nuceil and electronic.the potential energy quantum mechanically
\begin{subequations}
\begin{gather}
\begin{align}
  H(\mathbf{p}, \mathbf{q}) = \frac{\mathbf{p}^2}{2m} + V(\mathbf{q}),\quad
  \frac{\partial\mathbf{p}}{\partial t} = \frac{\partial V(\mathbf{q})}{\partial\mathbf{q}},\quad\quad \frac{\partial\mathbf{q}}{\partial t} = \frac{\mathbf{p}}{m}\quad\textrm{(classical Hamiltonian mechanics)},
\end{align}\\
  V(\mathbf{q}) = \sum_{i = 1} V_1(\mathbf{q}_i) + \sum_{i,j = 1} V_2(\mathbf{q}_i, \mathbf{q}_j) + \sum_{i,j,k = 1} V_3(\mathbf{q}_i, \mathbf{q}_j, \mathbf{q}_k) + \ldots \quad\textrm{(interatomic potential)}.
\end{gather}
\end{subequations}
Due to the limitation of computational power, the development of accurate interatomic potentials historically depended on the meticulous hand-tuning of parametric models called \emph{empirical interatomic potential}.
They were constrained by the chemical and phase space they could predict accurately~\cite{stillinger1985computer, tersoff1988empirical}.
However, with increased computational power, the shift towards data-driven models emerged allowing a more automatized construction fitting models on data from empirical potential~\cite{blank1995neural} or ab initio calculations~\cite{brown2003classical,lorenz2006descriptions,behl-parr07prl}.
Remarkably, current models have reached the capability to extensively cover chemical~\cite{lopanitsyna2023modeling} and phase space~\cite{bartok2018machine} accurately.

The abundance of machine learning (ML) packages available today~\cite{Haddad2023Artificial} prompts the question of the need for new ML packages for the development MLIP.
Most existing packages, however, are not suited to the specific needs of MLIP, which include domain-specific featurization of 3D structures and gradient inference with respect to target properties.
To ensure the code base remains manageable, strategic integration with existing software solutions is essential, wherever appropriate
%To keep development and maintenance efforts of the code base manageable it is necessary to integrate with existing software solutions where adequate.
This requires not only a solid understanding of relevant mathematical methods but also proficiency in current software tools.
This combination of requirements poses a significant challenge in the development of enduring and robust MLIP packages.
%To reduce the development and maintainance costs it is necessary to find points in ones method where already established software packages can be embedded in.
%For that a mathematical and algorithmic understanding of the landscape of methods and proficient skills in software development are needed which makes it such challenging problem.
 % tlerefore cannot be directly embedded for the development of MLIP.
%symmetrization of features based on point clouds in 3D
%symmetrized features in three-dimensional space 
%  are not requirements that are shared with many other domains that deploy ML models.
%This has be considered from multiple aspects does an interface happen without much loose in performance cost, is the maintainance cost of the interface lower than creating a implemantion.
%software development of MLIP remains challenging problem as most of the packages in the
%which is not a requirement that is not much shared with other domains.
%Secondly, the 
%the ML models rely on a featurization of data which is domain-dependent and therefore requires the development custom software.
%the creation of a model the shipping to an MD software.
%While featurization is domain-specific and needs custom implementations also the existing ML software ecosystem can often not to be reused for our domain as including gradients into the pictures completely changes the constraints on the required software.
%The gradient computation shifts the bottleneck drastically and thus changes the existing algorithmic design.

In this chapter I discuss my contributions to the software ecosystem that facilitates the deployment of MLIP into MD software.
These contributions include my work to the \texttt{librascal} package~\cite{LIBRASCAL}, instrumental for constructing MLIP based on the SOAP featurization of atomic structures, the \texttt{scikit-matter}~\cite{goscinski2023scikit} package a toolkit for various data-driven preprocessing methods that facilitate MLIP construction as feature and sample selection.
Additionally, I implemented an interface to \texttt{LAMMPS}~\cite{LAMMPS} enabling the studies on ferroelectric phase transitions in barium titanate, as detailed in Ref.~\cite{gigli2023modeling} and on the transport properties of lithium ortho-thiophosphate, as described in Ref.~\cite{gigli2023mechanism}.
Finally, we briefly discuss how the packages \texttt{equisolve}~\cite{equisolve} and \texttt{metatensor}~\cite{metatensor} enable a more modular approach to build MLIPs.

\section{Implementation of cubic splines for featurization}
\label{sec:cubic_spline}
%As part my work in the first year, I participated in the development of a library designed to allow efficient computation of atomic density based descriptors, named librascal.
%The library is designed with most emphasize on efficiency with regard to code structure and computation methods of descriptors.
%The code structure is focused on reducing runtime cost by allowing a contiguous iteration through atomic environments and by applying template based methods as curiously recurring template pattern (CRTP) to prevent runtime lookups of virtual functions with the tradeoff of higher compile time costs.
One of main contributions to \texttt{librascal} has been the implementation of a cublic spline to interpolate the radial expansion coefficients as expressed in Eq.~\eqref{eq:radial_expansion}.
%iin the last year has been the development of efficient methods for the computation of the radial basis expansion for the SOAP descriptor.
%In contrast to the approaches discussed in Section~\ref{sec:evaluation_of_radial_contribution}, for librascal Gaussian typed orbital (GTO) functions are used as radial basis functions 
%\begin{subequations}
%\begin{align}
%R_n^{GTO}(r) = r^{n+2}\exp(-b_nr^2)N_n,\hspace{5em}\\
%\text{with } b_n = 1/(2\sigma_n),\quad \sigma_n = r_c\,\textrm{max}(\sqrt{n},1)/n_{\text{max}},
%% Remark: I added the factor r^2 to the basis function to be comparable with other publications
%\end{align}
%\end{subequations}
%Performance test done with the descriptor library Dscribe show a speed up of $10.07\pm 0.56$ times of the GTO basis over the polynomial one\cite{himanen2020dscribe}.
%In librascal the GTO improve the expansion on the radial part is to separate the radial and angular part in the original atomic density function as proposed by Marco...
%that allowed further works
%the computation analytically the integral for the radial expansion in Eq.~\eqref{eq:radial_part_in_spherical_expansion}. 
%However, due to the modified spherical Bessel function $i_l$ %in Eq.~\eqref{eq:radial_part_in_spherical_expansion}
%an analytical expansion of radial term requires the evaluation of the computationally expensive confluent hypergeometric function $\hyponefone$ 
%\begin{multline}
%    \label{eq:radial_integral}
%    \int_0^{\infty} R_n^{GTO}(r) \exp[-a(r^2+r_{ij}^2)]i_l(2arr_{ij}) \,\mathrm{d}r
%    %\langle rlm|\mathcal{X}_i\rangle_{\hat{R}\hat{t}}\,\mathrm{d}r
%    = N_n \pi^{\frac32}\frac{\Gamma(\frac{n+l+3}{2})}{\Gamma(l+\frac32)} \\
%    (a+b)^{-\frac{n+l+3}{2}} (ar_{ij})^l \exp[-ar_{ij}^2] \hyponefone\big(\frac{n+l+3}{2},l+\frac32,\frac{a^2r_{ij}^2}{a+b}\big) = f^{nl}(r_{ij}).
%\end{multline}
%The contribution of this function to the overall cost of the radial expansion can be seen in Fig.~\ref{fig:hyp1f1_contribution} ranging from 55\% to 75\%  of the total time.F
For each coefficient $nl$, the one dimensional function $f^{nl}:[0,r_c]\rightarrow\mathbb{R}$ is splined.
The function $f^{nl}$ maps the distance $r\in[0,r_c]$ to the neighbor contribution of the radial expansion coefficient as in Eq.~\eqref{TODO}.
For the construction of the cubic spline the targeted interval $[0,r_c]$ is further partitioned into a set of subintervals $[r_1, r_2], \ldots, [r_{K}, r_{K+1}]$ where $0 = r_1 < r_2 < \ldots < r_{K} < r_{K+1} = r_c$.
Then cubic splines are order 3 polynomials $p_k(r) = A_k + B_kr + C_kr^2 + D_kr^3$ on the interval $[0,1]$ with the boundary conditions
%The radial expansion coefficients are evaluated for each of the distances $r_1 < r_2 < \ldots < r_{K+1}$ to set up a set of conditions
%The end points of each subinterval are evaluated with the origin approximated by a cubic polynomial boundary conditions.
%Let $\{r_k\}_{k=1}^{M+1}$ be the set of $M+1$ boundary points in the interval $[0,r_c]$, then cubic spline defines polynomials $p_k(r) = A_k + B_kr + C_kr^2 + D_kr^3$ on the interval $[0,1]$ with the boundary conditions
\begin{subequations}
\label{eq:abscissas_boundary_conditions}
\begin{gather}
    p^{nl}_{k}(0) = f^{nl}(r_k)\text{ and } p^{nl}_{k}(1) =  f^{nl}(r_{k+1})\text{ for } k=1,\ldots,K , \label{eq:function_boundary_conditions}\\
    %p^{nl}_{k}(0) = p^{nl}_{k-1}(1)&\text{ and }
    \left(\frac{\partial p^{nl}_{k}}{\partial r}\right)_{r=1} = \left(\frac{\partial p^{nl}_{k+1}}{\partial r}\right)_{r=0} \text{ for } k=1,\ldots,K-1  ,\label{eq:derivative_boundary_conditions}\\
    \left(\frac{\partial^2 p^{nl}_{k}}{\partial r^2}\right)_{r=1} = \left(\frac{\partial^2 p^{nl}_{k+1}}{\partial^2 r}\right)_{r=0} \text{ for } k=1,\ldots,K-1  ,\label{eq:second_derivative_boundary_conditions}\\
    %p^{nl}_k(1) = p^{nl}_{k+1}(0)&\text{ and }  \left(\frac{\partial p^{nl}_k}{\partial r}\right)_{r=1} = \left(\frac{\partial p^{nl}_{k+1}}{\partial r}\right)_{r=0} \text{ for } k=1,\ldots,K-1 ,\label{eq:derivative_boundary_conditions}\\
    \left(\frac{\partial^2 p^{nl}_{1}}{\partial r^2}\right)_{r=0} = 0\text{ and } \left(\frac{\partial^2 p^{nl}_{K}}{\partial r^2}\right)_{r=1}=0\quad\text{(natural boundary conditions)}.\label{eq:natural_boundary_conditions}
\end{gather}
\end{subequations}
%The boundary conditions ensure the continuity.
From these $4K$ boundary conditions a tridiagonal linear system can be formed solving for the $4K$ unknowns~\cite{bartels1998hermite}
%\begin{subequations}
%\begin{align}
%  \left(\frac{\partial p^{nl}_{k}}{\partial r}\right)_{r=0} &= B_k \\
%  \left(\frac{\partial p^{nl}_{k}}{\partial r}\right)_{r=1} &= B_{k+1} = B_k + 2C_k + 3D_k \\
%  \left(\frac{\partial^2 p^{nl}_{1}}{\partial r^2}\right)_{r=0} &= 0 = 2C_1 \\
%  \left(\frac{\partial^2 p^{nl}_{K}}{\partial r^2}\right)_{r=1} &= 0 = 2C_{K} + 6D_K \Rightarrow C_K = - 3D_K
%  p^{nl}(0) &= A_k \\
%  p^{nl}(1) &= A_{k+1} = A_k + B_k + C_k + D_k \\
%  \Rightarrow p^{nl}(1)-p^{nl}(0) &= B_k + C_k + D_k\\
%  \Rightarrow 3(p^{nl}(1)-p^{nl}(0)) &= 3B_k + 3C_k + 3D_k \\
%\end{align}
%\end{subequations}
%\begin{subequations}
%\begin{align}
%   0 = 2C_{k} + 3D_k
%  p^{nl}(1) - p^{nl}(0) k =
%  p^{nl}(0) &= A_k \\
%  p^{nl}(1) &= A_{k+1} = A_k + B_k + C_k + D_k \\
%  \left(\frac{\partial p^{nl}_{k}}{\partial r}\right)_{r=0} &= B_k \\
%  \left(\frac{\partial p^{nl}_{k}}{\partial r}\right)_{r=1} &= B_{k+1} = B_k + 2C_k + 3D_k
%\end{align}
%\end{subequations}
\begin{subequations}
\begin{align}
%A_k = f^{nl}(r_k),\quad C_k = 3(f^{nl}(r_{k+1})-f^{nl}(r_k)) - 2B_k-B_{k+1},\\
%D_k = 2(f^{nl}(r_{k})-f^{nl}(r_{k+1})) + B_k+B_{k+1},\hspace{5em}\\
\begin{pmatrix}
    2 & 1       &           &       &    \\
    1 & 4       & 1         &           &    \\
      & \ddots  & \ddots    & \ddots    &           \\
      &         & 1         &     4     & 1   \\
      &         &           &     1     & 2
\end{pmatrix}
\begin{pmatrix}
B_1\\
B_2\\
\vdots\\
B_{K-1}\\
B_{K}
\end{pmatrix}
=
\begin{pmatrix}
3(f^{nl}(r_2)-f^{nl}(r_1))\\
3(f^{nl}(r_3)-f^{nl}(r_1))\\
\vdots\\
3(f^{nl}(r_K)-f^{nl}(r_{K-2}))\\
3(f^{nl}(r_K)-f^{nl}(r_{K-1}))
\end{pmatrix}.
\end{align}
\end{subequations}
Such a systems can be solved in linear time with time complexity $O(2K)$ by iterating two times through the matrix following a Gaussian elimination scheme.
%\begin{equation}
%    
%\end{equation}
%In the forward pass the lower off diagonal is eliminated to zero, in the backward path the upper path is eliminated to zero.
%Commonly, for interpolators the grid size is adaptively changed until it is below an error bound given by the user as input.
The grid points $\{r_k\}_{k=0}^{K+1}$ are incrementally adjusted until the function approximation error falls beneath a user-specified error threshold.
The approximation error can be estimated by sampling points in the intervals $(r_k, r_{k+1})$ and computing the difference between the function $f^{nl}$ and the spline $p^{nl}$.
During implementation, it has been found that conditioning on both the relative and absolute error yields the most robust in terms of interpolation accuracy and convergence of the grid size.
While algorithms for adaptive grids can be advantageous to control the grid size and thus reduce memory requirements, it has been found that using a uniform grid does not reach a memory intensive regime while having minimal impact on the prediction accuracy.
This is evidenced by experiments on chemical shieldings of hydrogen environments~\cite{paruzzo2018chemical} presented in Fig.~\ref{fig:chemical_shift-spline}.
%as it can be seen in Fig.~\ref{TODO} with experiments on the chemical shieldings\cite{TOOD}.
%A point $r$ can be evaluated by first determining the corresponding boundary point $r_k$ for which $r\in[r_k,r_{k+1}]$, and then mapping $[r_k,r_{k+1}]$ to the polynomial interval $[0,1]$ with $p_k((r-r_k)/(r_{k+1}-r_k))$.
%For an arbitrary grid with $K+1$ points, the optimal complexity for searching the boundary point $r_k$ is $O(\log(M+1))$ with a binary searchc.
%\begin{equation}
%    k = \min(K,\lfloor (r-r_1)\Delta \rfloor+1), \quad \Delta = (K+1)/(r_{K+1}-r_1).
%\end{equation}
Compared to an adaptive grid the uniform grid reduces the asymptotic complexity of determining the subinterval for evaluation from logarithmic $O(\log K)$ binary tree search to constant $O(1)$ lookup.
Moreover, as the grid is constructed for each $nl$ channel, the setup and evaluation of the spline opens the possibility to parallelize the spline evaluation over all channels.
Viable forms of parallelization encompass multithreading, the use of Single Instruction Multiple Data (SIMD) instructions, or leveraging Graphics Processing Unit (GPU) acceleration.
%On the other hand for uniform grids the search time can be reduced to $O(1)$ by
%A non-uniform grid can keep the size of the grid to a minimum by only adding boundary points corresponding to the intervals with the highest error.
%This minimizes the number of evaluations of the costly function $f^{nl}$.
%With increasing number of evaluations of the function $f^{nl}$ the uniform grid will be always the better choice.
%Even though for realistic datasets with a limited number of evaluations this overhead could be crucial, it has been shown during implementation that the space overhead of a uniform grid is insignificant in comparison to the higher time complexity of the non-uniform grid.
%\begin{figure}
%    \centering
%    \includegraphics[width=0.5\textwidth]{fig/radial-basis.pdf}
%    \caption{Computational cost for the evaluation of the radial integral and its derivatives with different methods, for structures taken from the QM9 dataset. Note that the dataset has very little influence on this benchmark since the radial integral and its derivative are always evaluated once per neighbor. For the splining an accuracy of $10^{-8}$ was chosen}
%    \label{fig:radial-basis}
%\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{fig/spline-accuracy-v2.pdf}
    \caption{The relationship of the spline accuracy on the prediction difference for chemical shieldings of hydrogen environments as conducted in Ref.~\cite{paruzzo2018chemical} for a linear model using SOAP descriptors with $l_\textrm{max}=9$.
    The impact of the spline accuracy on the difference in prediction of the chemical shifts is several magnitudes below the DFT accuracy, which is estimated to be on the order of $10^{-1}$.}
    \label{fig:chemical_shift-spline}
\end{figure}
%\begin{figure}
%    \centering
%    \includegraphics[width=0.5\textwidth]{fig/fig_hyp1f1_contribution_in_radial_contribution.pdf}
%    \caption{}
%    \label{fig:hyp1f1_contribution}
%\end{figure}
%\begin{figure}
%    \includegraphics[width=\textwidth]{fig/slide20_2.png}
%    \caption{Effect of accuracy on prediction error}
%    \label{fig:spline-accuracy-prediction-error}
%\end{figure}\\
%\begin{figure}
%    \includegraphics[width=\textwidth]{fig/slide20_3.png}
%    \caption{Speedups}
%    \label{fig:spline-speedups}
%\end{figure}

\section{Interfacing with molecular dynamics packages}
%To make such simulations possible I implemented an interface with \texttt{LAMMPS} for our in-lab developed machine learning package \texttt{librascal} to exploit the implemented domain decomposition in \texttt{LAMMPS}.
%The correction of implementation was verified by comparing the trajectories with the MLIP package \texttt{QUIP}.
%The correctness of the interwork with LAMMPS domain decomposition was verified by running a simulation for different MPI tasks and comparing the results.
%This allowed obtain results...
Molecular dynamics (MD) packages, such as LAMMPS, GROMACS, CP2K, and i-PI~\cite{LAMMPS,hess+08jctc,kuhne2020cp2k,kapil2019pi}, commonly adopt the strategy of segregating the computation of the potential energy into a distinct module such that different potentials can be used.
As the computation of the potential energy only depends the atomic positions and type, and yields the energy and forces, it is a well-suited point for modularizing the code base.
One significant benefit of this approach is the avoidance of reimplementing established thermostats, barostats, and time integrators.
Although their implementation may appear straightforward, developing a robust code base transparently handles edge cases for the non-expert user is a time-consuming is nevertheless time-consuming task, demanding extensive documentation.
Furthermore, well-established MD software packages, such as \texttt{GROMACS}~\cite{abraham2015gromacs} and \texttt{LAMMPS}~\cite{LAMMPS}, offer a variety of parallelization strategies that significantly improve the speed of interatomic potentials.
These include MPI-based domain decomposition, CUDA- and OpenMP-based multithreading as well as SIMD abstraction modules for hardware-adaptive compute kernels.
The embedding of hardware dependent parallelization strategies, such as customized CUDA and compute kernels, necessitates adapting the interatomic pontential code to these specific kernel routines.
In contrast, MPI-based domain decomposition only requires dividing the potential into atomic contributions, as outlined in Eq.~\ref{eq:structural_separation}.
The forces then further evaluate to
\begin{equation}
  \label{eq:forces_interatomic_potential}
    -\frac{\partial E_A}{\partial\mathbf{r}_k}
  = -\sum_{i\in A} \frac{\partial E_i}{\partial\mathbf{r}_k}
  = -\sum_{i\in A}\sum_{j\in A_i} \frac{\partial E_i}{\partial\mathbf{r}_{ji}} \frac{\partial\mathbf{r}_{ji}}{\partial\mathbf{r}_k}
  \quad\textrm{, where }\frac{\partial\mathbf{r}_{ji}}{\partial\mathbf{r}_k} = \begin{cases}1,& k=i \\ -1,& k=j \\0,& \textrm{ else.} \end{cases}
\end{equation}
It is evident that the forces acting on atom $i$ depend solely local energies within its neighborhood.
Consequently, these forces can be embarrassingly parallelized by dividing the cells into domains, with each one assigned to an independent hardware component.
Communication between the domains becomes necessary only when the partial forces $-\partial E_i/\partial\mathbf{r}_{ji}$ for atoms that fall within the neighborhoods of multiple domains are communicated instead of recomputed, see for example the parameter \emph{newton} in the \texttt{LAMMPS} manual~\cite{lammpsnewton}.
Global communication is required solely for aggregating the local energy contributions into the total energy, as specified by the \emph{nstcalcenergy} parameter in the \texttt{GROMACS} manual~\cite{gromacsnstcalcenergy}.
In both cases, the implementation of these communication can abstracted out of the potential code, thereby enabling a modular design that does not need to account for these specific details.
In the case of the study on barium titanate~\cite{gigli2023modeling}, the MPI parallelization of \texttt{LAMMPS} played a crucial role in investigating the impact of long-range dielectric correlations on the Curie temperature, due to the necessity of conducting simulations on large cell sizes.
%However, this domain decomposition demands careful consideration in the implementation of gradients.
%The nature of short-range interatomic potentials,  whih decompose total energy into contributions from local environments, facilitates the division of the cell into smaller domains.
%The potential energy calculations for each of these domains can then be distributed across multiple processors or machines, utilizing a message passing interface (MPI).
%%The domain decomposation requires particular considerations in the implementation of the gradients.
%The force computation for an interatomic potential can be computed as
%%The evaluation of gradients using the neighbor list approach is expressed
%\begin{equation}
%  \label{eq:forces_interatomic_potential}
%  \frac{\partial E_A}{\partial\mathbf{r}_k} = \sum_{i\in A} \frac{\partial E_i}{\mathbf{r}_k} = \sum_{i\in A}\sum_{j\in A_i} \frac{\partial E_i}{\mathbf{r}_{ji}} \frac{\mathbf{r}_{ji}}{\mathbf{r}_k}\textrm{, where }\frac{\mathbf{r}_{ji}}{\mathbf{r}_k} = \begin{cases}1,& k=i \\ -1,& k=j \\0,& \textrm{ else.} \end{cases}
%\end{equation}
%%Without domain decomposition neighbors outside of the cell are periodic images and partial forces do not need to be assigned as they can be ignored.
%%With domain decomposition neighbors outside of the cell can also be nonperiodic neighbors part of a different domain.
%%\papercomment{Here I could talk about the implementation detail how to add neighbor contributions}
%It can be seen that the forces at position $i$ depend on the partial forces wrt. to energy $i$ and all its neighbors $j\in A_i$.
%Depending on memory storage of partial forces this case needs to be handled with care.
%In particular considering the fact that MD software as \texttt{LAMMPS} offers the option to allow MPI communication between the domains with the \emph{newton\_pair} option.
%Keeping the option on prevents redundant computations of the forces but requires more MPI communication as the partial forces need to be communicated.
%In \texttt{librascal} if atom $j$ is a periodic neighbor, the acces on atom $j$ remapped to corresponding atom in the box.
%The MPI communication requires to differ between periodic neighbors and ghost atoms that are part of another domain.
%One can simplify this complexity storing the partial forces $\partial{E_i}/\partial{\mathbf{r}_{ij}}$ and $\partial{E_j}/\partial{\mathbf{r}_{ij}}$ for redundantly for atom $i$ and $j$.
%This is redundant due to Netwton's third law $\partial{E_j}/\partial{\mathbf{r}_{ij}} = - \partial{E_j}/\partial{\mathbf{r}_{ji}}$.
%While being redundant it does not change asympotic scaling of the memory requirements for the forces.
%i, ij
%j, ji
%i, ji
%j, ij
%where i, ij = - i, ji

\section{Implementation of gradients in kernel models}
%As invariant 3-body descriptors are easy to evaluate from the spherical expansion coefficients without requiring additional dependencies to compute the Clebsch-Gordan coefficients as higher-body orders need and as their the memory consumption hits a sweet spot for the current generation of hardware, they have been a popular descriptor in applications.
%As their accuracy in combination with linear models is often not sufficient enough to produce accurate enough molecular dynamics simulations, one common approach has been to incorperate kernel models instead.
We do not give a pedagological introduction to kernel methods, but only discuss the specificalities that arise when extending them for gradients prediction.
For a comprehensive introduction to kernel models please refer to Ref.~\cite{bishop2006pattern}.

For a kernel $k$ fitted on the samples $\{\mathbf{c}_t\in\mathbb{R}^M\}_{t=1}^N$ and targets $\{y_t\in\mathbb{R}\}_{t=1}^N$ the optimal weights $\boldsymbol{\alpha}\in\mathbb{R}^N$ are retrieved as solution of the minimization problem
\begin{subequations}
  \label{eq:minkernel}
  \begin{gather}
    \boldsymbol{\alpha} = \underset{\boldsymbol{\alpha}^\prime\in\mathbb{R}^N}{{\operatorname{argmin}}}\,\, \ell(\boldsymbol{\alpha}^\prime)\\
    \ell(\boldsymbol{\alpha}) = \sum_{n}^N\| y_{n} - \sum_{t=1}^N \alpha_t k(\mathbf{c}_t, \mathbf{c}_{n}) \|^2
  \end{gather}
\end{subequations}
can be expressed in vector form as
\begin{subequations}
  \label{eq:minkernel}
  \begin{align}
    \ell(\boldsymbol{\alpha}) = \|\mathbf{y} - \mathbf{K}\boldsymbol{\alpha}\|^2 \\
  \end{align}
\end{subequations}
that can be easily solved setting the derivative to zero
\begin{subequations}
  \label{eq:solving_kernel}
  \begin{align}
    0 &= \frac{\partial \ell(\boldsymbol{\alpha})}{\partial\boldsymbol{\alpha}}\\
    0 &= 2\mathbf{K}^T\mathbf{K}\boldsymbol{\alpha} - 2\mathbf{K}\mathbf{y}\\
    \mathbf{K}\mathbf{y} &= \mathbf{K}^T\mathbf{K}\boldsymbol{\alpha}\\
    \mathbf{K}\mathbf{y} &= \mathbf{K}\mathbf{K}\boldsymbol{\alpha},\textrm{ since $\mathbf{K} = \mathbf{K}^T$} \label{eq:kernel_symmetry}
  \end{align}
\end{subequations}
which gives $\mathbf{K}^{-1}\mathbf{y}$ as solution for $\boldsymbol{\alpha}$.
%(see 6.7 https://people.eecs.berkeley.edu/~wainwrig/stat241b/lec6.pdf 
The solution can subsequently be used to evaluate on an arbitrary point $i$ by the relationship
\begin{equation}
  \label{eq:kernel_evaluation}
  \sum_{t=1}^N \alpha_t k(\mathbf{c}_t, \mathbf{c}_i) = y_i.
\end{equation}
Note that in principle the solution does not resolve in a exact solution, especially considering regularization, but we omitt this detail for simplicity of equations. 
%The solution to the problem \ref{eq:minkernel} can be expressed in matrix form as
%\begin{equation}
%  \boldsymbol{\alpha} = (\mathbf{K} + \boldsymbol{\Lambda})^{-1}\mathbf{y}.
%\end{equation}
\subsection{Kernel evaluation}
Now to include the gradients wrt. to the atomic position $\mathbf{r}_k$ of atom $k$ into the picture, we first note that the training points used to construct the kernel are independent from the points for which gradients are evaluated.
In other words they are therefore independent to changes in $\mathbf{r}_k$ for any structure when evaluating~\ref{eq:kernel_evaluation}.
We can therefore use the notation $k_t(\mathbf{c}_i)$ to denote $k(\mathbf{c}_t, \mathbf{c}_i)$ for easier readability of the derivatives.
The partial force can then be expressed as
\begin{subequations}
\label{eq:kernel_forces}
\begin{align}
  \frac{\partial E_i}{\partial\mathbf{r}_{ji}}
     &= \sum_{t=1}^N \frac{\partial \alpha_t k_t(\mathbf{c}_i)}{\partial\mathbf{r}_{ji}} \\
     &= \sum_{t=1}^N \alpha_t \frac{k_t(\mathbf{c}_i)}{\partial\mathbf{c}_i} \frac{\partial\mathbf{c}_i}{\partial\mathbf{r}_{ji}} \\
     %&= \frac{\partial\mathbf{c}_i}{\partial\mathbf{r}_{ji}} \sum_{t=1}^N \alpha_t \frac{\partial k_t(\mathbf{c}_i)}{\partial\mathbf{c}_i}. \label{eq:efficient_gradient_eval}\\
  %\frac{\partial E_i}{\partial\mathbf{r}_{k}} &= \sum_{j\in A_i}\frac{\partial E_i}{\partial\mathbf{r}_{ji}}
    %&= \sum_{j\in A_i}\sum_n \alpha_n \frac{k_n(\mathbf{c}_i)}{\partial\mathbf{c}_i} \frac{\partial\mathbf{c}_i}{\partial\mathbf{r}_k} \frac{\partial c_i}{\partial\mathbf{r}_k}
\end{align}
\end{subequations}
which gives us the final expression
\begin{equation}
  \frac{\partial E}{\partial\mathbf{r}_k} = \sum_{t=1}^N \alpha_t \underbrace{\sum_{i\in A} \sum_{j\in A_i}\frac{\partial k_t(\mathbf{c}_i)}{\partial\mathbf{c}_i}\frac{\partial\mathbf{c}_i}{\partial\mathbf{r}_{ji}}\frac{\partial\mathbf{r}_{ji}}{\partial\mathbf{r}_k}}_\textrm{i,k kernel force component}
\end{equation}
One can include the gradients wrt. to the training points in the original expression and get
\begin{equation}
  \label{eq:kernel_evaluation}
  \sum_{t=1}^N \alpha_t k(\{\mathbf{c}_t\}_{t\in A}, \{\mathbf{c}\}_{n\in A}) = E.
\end{equation}
and get
\begin{equation}
  \label{eq:kernel_evaluation}
  \sum_{t=1}^N \alpha_t k(\{\mathbf{c}_t\}_{t\in A}, \{\mathbf{c}\}_{n\in A}) 
  %\sum_{(t,p)=1}^{\partial\!\! N} \alpha_n k(\{\mathbf{c}_(t,p)\}_{t\in A}, \{\mathbf{c}\}_{n\in A})
  + \sum_{t\in A^\prime} \alpha_t \sum_{j^\prime\in A^\prime_t}\frac{\partial k(\mathbf{c}_t, \mathbf{c}_i)}{\partial\mathbf{c}_t}\frac{\partial\mathbf{c}_t}{\partial\mathbf{r}^\prime_{j^\prime t}}\frac{\partial\mathbf{r}^\prime_{jt}}{\partial\mathbf{r}^\prime_{k^\prime}} \\
     = E.
\end{equation}
Then we can solve for the loss

It can be seen that wa part that can be factored out of the weights is contributing to the
%(TODO this needs to be written at the end) Note that extracting the partial feature gradients out of the sum reduces the complexity from $O(MN)$ to $O(M+N)$ when evaluating for gradients after the training.
We note that including gradients into the training also changes the evaluation of the kernel entries
\begin{subequations}
\begin{align}
  %k\Large(\frac{\partial\mathbf{c}_t}{\partial\mathbf{r}^\prime_k}, \mathbf{c}_i\Large) &= 
  k\Large((t,k^\prime), \mathbf{c}_i\Large) &= 
  \sum_{t\in A^\prime} \sum_{j^\prime\in A^\prime_t}\frac{\partial k(\mathbf{c}_t, \mathbf{c}_i)}{\partial\mathbf{c}_t}\frac{\partial\mathbf{c}_t}{\partial\mathbf{r}^\prime_{j^\prime t}}\frac{\partial\mathbf{r}^\prime_{jt}}{\partial\mathbf{r}^\prime_{k^\prime}} \\
  \frac{\partial k\Large((t,k^\prime), \mathbf{c}_i\Large)}{\partial \mathbf{r}_{k}} %&= 
    %\sum_{t\in A^\prime} \sum_{j^\prime\in A^\prime_t}\frac{\partial k(\mathbf{c}_t, \mathbf{c}_i)}{\partial\mathbf{c}_t}\frac{\partial\mathbf{c}_t}{\partial\mathbf{r}^\prime_{j^\prime t}}\frac{\partial\mathbf{r}^\prime_{j^\prime t}}{\partial\mathbf{r}^\prime_k} \sum_{i\in A} \sum_{j\in A_i}\frac{\partial k_t(\mathbf{c}_i)}{\partial\mathbf{c}_i}\frac{\partial\mathbf{c}_i}{\partial\mathbf{r}_{ji}}\frac{\partial\mathbf{r}_{ji}}{\partial\mathbf{r}_k}\\
    &= \sum_{i\in A}\sum_{j\in A_i}\sum_{t\in A^\prime} \sum_{j^\prime\in A^\prime_t}\frac{\partial^2 k(\mathbf{c}_t, \mathbf{c}_i)}{\partial\mathbf{c}_t\partial\mathbf{c}_i}\frac{\partial\mathbf{c}_t}{\partial\mathbf{r}^\prime_{jt}}\frac{\partial\mathbf{r}^\prime_{jt}}{\partial\mathbf{r}^\prime_{k^\prime}} \frac{\partial\mathbf{c}_i}{\partial\mathbf{r}_{ji}}\frac{\partial\mathbf{r}_{ji}}{\partial\mathbf{r}_k}\\
    %\frac{\partial k\Large(\frac{\partial\mathbf{c}_t}{\partial \mathbf{r}_k}, \mathbf{c}_i}{\partial ...}\Large) = 
    %  \sum_{i\in A} \sum_{j\in A_i}\frac{\partial k_t(\mathbf{c}_i)}{\partial\mathbf{c}_i}\frac{\partial\mathbf{c}_i}{\partial\mathbf{r}_{ji}}\frac{\partial\mathbf{r}_{ji}}{\partial\mathbf{r}_k} \\
\end{align}
\end{subequations}
This adds an additional complexity which we hide away by using the notation $k_t(\mathbf{c}_n)$ as we will see later that people typically ignore the gradient part as training points when using a low-rank approximation. 

%k(\frac{\partial\mathbf{c}_t}{\partial r_k^{(p)}}, \mathbf{c}_i)

\subsection{Kernel fitting}
We can include the forces into the optimization problem using Eqs.~\ref{eq:kernel_forces} and~\ref{eq:forces_interatomic_potential}
%\begin{multline}
%  \label{eq:loss_forces}
%\ell_n = 
% \ell(\boldsymbol{\alpha}) = 
%                      \sum_{n}^N \| 
%                      E^{(n)} - \sum_{t=1}^N \alpha^\prime_t k_t(\mathbf{c}_{n}) + \sum_{t=1}^{\grad N} \beta_t k_{\partial t}(\mathbf{c}_{n})\|^2
%                         \\ + \sum_{k\in A^{(n)}} \|\frac{\partial E^{(n)}}{\partial\mathbf{r}_{k}} 
%                             - \big(
%                                 \sum_{t=1}^N \alpha_t \sum_{i\in A^{(n)}}
%                                 \sum_{j\in A^{(n)}_i}\frac{\partial k_t(\mathbf{c}_i)}{\partial\mathbf{c}_i}
%                                 \frac{\partial\mathbf{c}_i}{\partial\mathbf{r}_{ji}}\frac{\partial\mathbf{r}_{ji}}{\partial\mathbf{r}_k}
%                                 \\ +
%                                 \sum_{t=1}^{\grad N} \beta_t \sum_{i\in A^{(n)}}
%                                 \sum_{j\in A^{(n)}_i}\frac{\partial k_{\partial t}(\mathbf{c}_i)}{\partial\mathbf{c}_i}
%                                 \frac{\partial\mathbf{c}_i}{\partial\mathbf{r}_{ji}}\frac{\partial\mathbf{r}_{ji}}{\partial\mathbf{r}_k}
%                               \big)
%                        \|^2.
%\end{multline}

...
\begin{multline}
  \label{eq:loss_forces}
%\ell_n = 
 \ell(\boldsymbol{\alpha}) = 
                      \sum_{n}^N \| 
                         E^{(n)} - \sum_{t=1}^N \alpha^\prime_t k_t(\mathbf{c}_{n}) \|^2
                         + \sum_{k\in A^{(n)}} \|\frac{\partial E^{(n)}}{\partial\mathbf{r}_{k}} 
                             - \sum_{t=1}^N \alpha_t \sum_{i\in A^{(n)}} \sum_{j\in A^{(n)}_i}\frac{\partial k_t(\mathbf{c}_i)}{\partial\mathbf{c}_i}\frac{\partial\mathbf{c}_i}{\partial\mathbf{r}_{ji}}\frac{\partial\mathbf{r}_{ji}}{\partial\mathbf{r}_k}
                        \|^2.
\end{multline}
We use $A^{(n)}$ to index structures as we use $A_n$ to index the atom within a structure.
Unlike in Eq.~\ref{eq:efficient_gradient_eval} the $\alpha_t$ has been extracted out to see more clearly that the loss function can be translated into the vectorial form
\begin{subequations}
  \begin{gather}
    \ell(\boldsymbol{\alpha}) = \|\mathbf{K}\boldsymbol{\alpha} - \mathbf{y}\|^2, \\
    \textrm{with }\quad \mathbf{K} =
    \left[
      \begin{array}{cc}
        \mathbf{K}_{N,N} & \mathbf{K}_{N, \partial\!\! N}\\
        %\hline
        \mathbf{K}_{\partial\!\! N,N} & \mathbf{K}_{\partial\!\! N, \partial\!\! N}
      \end{array}
    \right]
    \quad\textrm{and }\quad \mathbf{y} = 
    \left[
      \begin{array}{c}
        \mathbf{E} \\
        %\hline
        \frac{\partial \mathbf{E}}{\partial \mathbf{r}_k}
      \end{array}
    \right],\\
    \mathbf{K}_{NN}\in\mathbb{R}^{N,N},\qquad \mathbf{K}_{\partial\!\! NN}\in\mathbb{R}^{\partial\!\! N,N},\qquad \partial\!\! N = 3\sum_n |A^{(n)}|,\\
    \mathbf{E}\in\mathbb{R}^N,\qquad \frac{\partial\mathbf{E}}{\partial\mathbf{r}_k}\in\mathbb{R}^{\partial\!\! N}
  \end{gather}
\end{subequations}
where $\mathbf{K}_{\grad NN}$ is the matrix from the stacked sum-product of the feature and kernel gradients in the loss~\ref{eq:loss_forces}, explicitely written
where $\partial N$ denotes the number stacked gradients for each Cartesian dimension $3\sum_n |A^{(n)}|$
and $\mathbf{K}_{\grad NN}$ is the matrix from the stacked sum-product of the feature and kernel gradients in the loss~\ref{eq:loss_forces}, explicitely written
\begin{subequations}
  \begin{align}
    [\mathbf{K}_{\grad NN}]_{(n,k,p),t} = \sum_{i\in A^{(n)}} \sum_{j\in A^{(n)}_i}\frac{\partial k_t(\mathbf{c}_i)}{\partial\mathbf{c}_i}\frac{\partial\mathbf{c}_i}{\partial r^{(p)}_{ji}}\frac{\partial r_{ji}^{(p)}}{\partial r_k^{(p)}},
  \end{align}
\end{subequations}
where $(n,k,p)$ is the flattened multi-index specifiyng the structure, the atom and the Cartesian dimension.
%Note that $\mathbf{K}$ is in this case not symmetric and therefore cannot be solved the usual way as shown in Eq.~\ref{eq:kernel_symmetry}.
%One can still derive a solution from Eq.~\ref{eq:solving_kernel} as it is done for linear regression,
%\begin{equation}
%  \boldsymbol{\alpha} = (\mathbf{K}^T\mathbf{K})^{-1}\mathbf{K}^T\mathbf{Y},
%\end{equation}
The inclusion of the gradients explodes the size of the memory requirements for the kernel matrix, because the $\partial N$ grows with total number of atoms instead of structures like the energies.
A fruitful approach has been therefore to perform a low-rank approximation of the kernel matrix~\cite{silverman1985some,williams2000using,smola2000sparse,quinonero2005unifying} by projecting the $3\sum_n N_n$ points onto a fixed number of representative points $\{\tilde{c}_t\}_{t=1}^T$, we refer as \emph{pseudo points} as used in referencei~\cite{quinonero2005unifying}.
In the Bayesian community the low-rank approximation has established the naming \emph{subset of regressor}~\cite{quinonero2005unifying} while the frequentist community it is more commonly referred as Nyström approximation~\cite{williams2000using}.
\begin{subequations}
\begin{align}
  \tilde{\mathbf{K}} = \mathbf{K}_{TT} + \mathbf{K}_{T,N + \partial N}\mathbf{\Lambda}^{-2}\mathbf{K}_{T,N + \partial N}^T,
\end{align}
\end{subequations}
An established way has been to only use single envirnoments as pseudo points and to dismiss gradient contributions in them.
One advantage is that this skips an expensive evaluation of kernel entries using gradients as training points.
The additional advantage is that environmental information is often redundantly present within structures, thus a much smaller "represantative" set can be found reducing the evaluation time of one kernel entry from $N_n^2$ to $N_n$.
%The low-rank approximation contributes greatly in the reduction of the computation of these kernel entries as it projects the structural features on single $M$ environmental features resulting in a linear scaling of on kernel entry.
%Since the number of atoms in structures is a substantial quantity and the fact that structures typically contain redundant environments, the rank can be reduced quite significantly making this approach indispensable.
%
%We can further 
%%  = alpha^2 - 2alphak*y + alphak + 
%%  (XX+X_FX_F)^{-1} (XY + X_FF)
%%   => K= XX+X_FX_F
%\begin{subequations}
%  \label{eq:.}
%  \begin{align}
%  \sum_{k\in A^{(n)}} \|\frac{\partial E}{\partial\mathbf{r}_{k}}  
%                               - \sum_{i\in A^{(n)}} \sum_{j\in A^{(n)}_i}\frac{\partial\mathbf{c}_i}{\partial\mathbf{r}_{ji}} \sum_{t=1}^N \alpha^\prime_t \frac{\partial k_t(\mathbf{c}_i)}{\partial\mathbf{c}_i}\frac{\partial\mathbf{r}_{ji}}{\partial\mathbf{r}_k}
%  \end{align}
%\end{subequations}
%
%We can extract the alphas out 
%\begin{equation}
%   \sum_{t=1}^N \alpha^\prime_t \sum_{i\in A^{(n)}} \sum_{j\in A^{(n)}_i}\frac{\partial\mathbf{c}_i}{\partial\mathbf{r}_{ji}} \frac{\partial k_t(\mathbf{c}_i)}{\partial\mathbf{c}_i} %= \sum_{t=1}^N \alpha^\prime_t \nabla k_t(\mathbf{c}_i)
%   %\frac{\partial k_t(\mathbf{c}_i)}{\partial\mathbf{r}_{k}}
%\end{equation}
%which solves to a kernel of form system of
%\begin{equation}
%  %\sum_{t=1}^N \alpha^\prime_t \sum_{i\in A^{(n)}} \sum_{j\in A^{(n)}_i}\frac{\partial\mathbf{c}_i}{\partial\mathbf{r}_{ji}} \frac{\partial k_t(\mathbf{c}_i)}{\partial\mathbf{c}_i}
%  \grad k_t(c_i) = k_t(\mathbf{c}_{i}) 
%                      %+ \sum_{i\in A^{(i)}} \sum_{j\in A^{(n)}_i}\frac{\partial\mathbf{c}_i}{\partial\mathbf{r}_{ji}} \frac{\partial k_t(\mathbf{c}_{i}) }{\partial\mathbf{c}_i} 
%  + \sum_{i\in A^{(n)}} \sum_{j\in A^{(n)}_i}\frac{\partial\mathbf{c}_i}{\partial r^{d}_{ji}} \frac{\partial k_t(\mathbf{c}_i)}{\partial\mathbf{c}_i}
%\end{equation}
%Which ends up in the common formulation of stacking the features and gradient featuers to obtain a matrix~\cite{willattcsanyTODO}
%\begin{equation}
%  \tilde{K} = \begin{pmatrix}K \\ \grad K \end{pmatrix}; \tilde{K}^T\tilde{K}
%\end{equation}
%
%One can see that the gradient evaluation depends per atom property, thus the kernel matrix is growing with respect to number of atoms in all training structures which increase memory consumption even for small systems by a factor of 10.
%%A popular approach that works with current hardware is the sun
% % into the evaluation.
%%For that reason for the work $BaTiO_3$ a kernel model was used.
%Since gradients make the evaluation of the full kernel matrix computationally costly, low-rank approximation techniques have become indispensable early on to reduce the memory intensive usage during training.
%The subset of regressor method\cite{quinonero2005unifying} has been a popular low-rank estimation used in the MLIP packages \texttt{QUIP}\cite{Csanyi2007-py}.
%
%The core idea is to project the data points on a subset of the $M$ pseudo points.
%\begin{subequations}
%\begin{align}
%  \mathbf{K} = \mathbf{K}_{MM} + \mathbf{K}_{MN}\mathbf{\Lambda}^{-2}\mathbf{K}_{MN}^T
%\end{align}
%\end{subequations}
%Note that to compute one kernel entry for two structures with each $N_i$ atoms it requires $N_i^2$ evaluations
%\begin{subequations}
%\begin{align}
%  k^A(A_{i}, A_{i^\prime}) = \sum_{j\in A_i}^{N_i}\sum_{j^\prime\in A_{i^\prime}}^{N_{i^{\prime}}} k(\mathbf{c}_j, \mathbf{c}_{j^\prime}).
%\end{align}
%\end{subequations}
%The low-rank approximation contributes greatly in the reduction of the computation of these kernel entries as it projects the structural features on single $M$ environmental features resulting in a linear scaling of on kernel entry.
%Since the number of atoms in structures is a substantial quantity and the fact that structures typically contain redundant environments, the rank can be reduced quite significantly making this approach indispensable.

\subsection{Moduler kernel computation}
One disadvantage in the design of \texttt{librascal} in the entanglement of the featurization and the model building to the libraray.
This was required as the construction of the kernel matrix requires understanding by the model building tool of the decomposition of the target property into local contributions Eq.~\eqref{eq:structural_separation} as well as what its gradients are, domain-agnostic packages as \texttt{scikit-learn} are not suitable for a direct application in this case.


The software package \texttt{metatensor} allows to attribute these structural characteristics to the object itself as metadata and offers \texttt{numpy}-like data manipulations that take advantage of the metadata.
This allowed a disentanglement of the featurization that has reimplemented in a new package \texttt{rascaline} and model construction that has been moved to \texttt{equisolve}.
Part of my contribution was to participate in the development of \texttt{metatensor} as well as heavily developing on the initial design of \texttt{equisolve} contributing module of shallow methods including standardizer, linear and the above-presented kernel model as well as the inital designs for neural network models based on the data formate created in \texttt{metatensor}.

While the term $\partial\mathbf{c}_i/\partial\mathbf{r}_k$ depends solely on the choice of featurization, the term $\partial k_t(\mathbf{c}_i)/\partial\mathbf{c}_i$ depends on the choice of kernel, thus the computation of the kernels and featurizations can be separated.
A problem that arised in \texttt{librascal} is that the species introduce a high structural sparsity in the features that needs to be also considered in the kernel computation to be efficient.
For that the kernel needs to support such a sparsity.
While there exist multiple sparsity formats\cite{}, they are agnostic to the structure of the data and thus do not allow us to exploit structural sparsities that depend on specif information as the species.
To solve  was the implementation of metatensor.


%Due to the structural nature of samples in atomistic learning, one sample consists of features for each atom presen in the structure, the  has been a popular approach as it allows a selection of individual atomic features as pseudo points unlike to the more common low-rank Nyström approach where whole structures would need to be selected.
%The partial forces in the kernel model can this be evaluated by 
%\begin{subequations}
%\begin{align}
%  E_i = \partial \sum_{j\in A_i} \sum_{t\in T} \alpha_k k(x_t, x_j) \\
%  \frac{\partial E_i}{\mathbf{r}_{ji}} = \sum_{j\in A_i} \sum_{t\in T} \alpha_k \frac{k(x_t, x_j)}{\mathbf{r}_{ji}} \\
%   \frac{k(x_t, x_j)}{\mathbf{r}_{ji}} = \frac{x_j}{\mathbf{r}_{ji}}
%\end{align}
%\end{subequations}

%Custom optimizer

%\papercomment{One could include the work on solvers here but it is note so important efficiency}

\section{A metadynamic software framework with LAMMPS, PLUMED and i-PI}
To study phase transitions of barium titanite in Ref.~\cite{gigli2023modeling} we needed to accelerate the sampling of the transition.
One common technique that we use is metadynamics that adds a bias potential to the simulation that is later normalized out in the calculation of the free energy.
While the forces of the MLIP were computed with \texttt{LAMMPS} and the forces of the bias potential were computed with \texttt{PLUMED}\cite{PLUMED}.
To consolidate both forces for the metadynamics the software-package \texttt{i-PI} was used, that implemented a custom protocol to each of these MD packages to allow communicatiof the forces to a python interface.
A schematic of the interwork between the software packages can be seen in Fig.~\ref{fig:ipi-librascal-plumed}.
\begin{figure}
    \includegraphics[width=\textwidth]{fig/ipi-librascal-plumed.pdf}
    \caption{A schematic showing the interwork of the software pieces to run metadynamic simulations to study interfacial effects of barium titanite.}
    \label{fig:ipi-librascal-plumed}
\end{figure}

As CV for the metadynamics $l=1$ components of the spherical expansion coefficients computed with \texttt{librascal} were used.
My implementation of the cubic spline helped to speed up the computation of the bias term.
Since only the expansion coefficients centered for the oxygen neighbors centered on the titanite was sufficient for the CV, I further implemented an option to selectively computate the partial gradients for certain species.
Note that a selectional computation of partial gradients also needs to consider dependencies of the gradient on the central energies of its neighbors as pointed out in Eq.~\eqref{eq:forces_interatomic_potential}.
%implementation of was implemented by me to speed up the metadynamics simulation
%work are adjustments to the
%In the subsequent sections we will discuss the technica
%My contributions to this work was firstly, to participate in the development of the MLIP package \texttt{librascal} that was used to conduct DFT simulations at higher levels-of-theory to analyse its effect on the temperature.
%Specifically, the implementation of selective computations of the partial gradients for certain species was implemented by me to speed up the metadynamics simulation.
%Secondly, the implementation of an interface to the MD software \texttt{LAMMPS} that allowed to study the finite size effects on the Curie temperature.

%The interface was implemented by Gareth Tribello.

%\section{Modular design of MLIP}
%As scientific methods continue to evolve, the demands placed on software to incorporate these advancements also grow.
%%While advances in methodolgy can prove to improve a method in all aspects, and thus replace an existing method, it is more often the case that they extend the capabilities and requiring more flexibility of the software outside of the initial design of the software package.
%While improvements in method development often lead to enhanced efficiency and user-friendliness, they typically extend the software's capabilities beyond its original design.
%As software is extended by more features over time, it tends to become less adaptable to changes to incorperate newly developed methods.
%Consequently, rapid advancements in methodology can render software obsolete before the development is even at the stage of deployment for experiments.
%%Considering that software becomes more rigid to changes over time with increased number of implemented features, it is a natural consequence that with a rapid development in methodology the software becomes deprecated as it cannot keep up with these changes.
%Similar challenges were observed in the 1960s when rapid hardware development contributed to a software crisis\cite{brian2012software_crisis}.
%However, as development progresses, also the understanding of critical components for performance emerges which solidifies the possible software designs to retrieve performant code.
%With this places in the software naturally appear that can be used to decompose it into separate components allowing to distribute the software development workload among peers.
%%groups as they can independently work on the individual modules.
%This not only allows to speed up the development but it also allows a more specialized development leading to more performant code.
%While scientific publications cover abundantly method development based on mathematical advancements, the modular design of software implementing a variety of methods in a specific domain is still highly underrepresented in the scientific literature even though it dictates the efficiency the method development.
%%With regard to incensitives present academia that only fund software development tied to new scientific applications, the existing fragmented landscape of unmaintained monolothic software packages solving over and over again the same problem is a natural consequence.
%%Advances can be only made when enough money is allocated to a group that recognizes the need to create a software package standardizing interfaces.
%This section discusses modular design patterns relevant for the domain of atomistic learning to integrate of machine learning interatomic potentials with molecular dynamics packages.
%Interfaces where applicable are needed to reduce software costs for groups focusing on optimization of .
%Modular design is essential for sustainable scientific problem: Not reinventing the wheel, extensibility, wider-applicability, focused problem-solving
%The production of reliable and maintained software is essential for the progress of further scientific development to  prevent a scientfic crisis as in the \cite{TODO}.
%
%%\section{MLIP}
%%A common approach for the integration of machine learning into the molecular dynamics simulation
%%\begin{equation}
%%  H(\mathbf{p}, \mathbf{q}) = \frac{\mathbf{p}^2}{2m} + V(\mathbf{q})% + theromstat/barostat
%%\end{equation}

%Note that contributions to the gradient come from the partial gradients $\partial E_k/\mathbf{r}_{jk}$ 
%wrt. to the distance vector to all neighbors 
%\emph{and} from $\partial E_j/\mathbf{r}_{kj}$ where $j$ is in the neighborhood of atom $k$ ($j\in A_k$). 
%additional pr to be done to allow contiguous iteration during the computation of the gradients.

%Note that one can simplify the gradients for further usage by summing neighbour and central contributions
%\begin{subequations}
%\begin{align}
%  \mathbf{F}_{kj} = -\frac{\partial E_k}{\mathbf{r}_{jk}} + \frac{\partial E_j}{\mathbf{r}_{kj}} \\
%  \frac{\partial E_A}{\partial\mathbf{r}_k} = \sum_{j\in A_k} \mathbf{F}_{kj}.
%\end{align}
%\end{subequations}

%\cite{https://docs.lammps.org/Developer_write_pair.html}
%As classical molecular dynamics simulation require the computation

%In this section we will discuss the approach that is taken in software for classical molecular dynamics code as \cite{LAMMPS, Gromacs, CP2K, Plumed, Jaxmd, OpenMMTODO}.
%%equilibrium hat uses the Born-Oppenheimer approximation 
%%software simulating equilibrium dynamics using the Born-Oppenheimer approximation so 
%%equilibrium hat uses the Born-Oppenheimer approximation 
%%A common approach for equilibrium molecular dynamics software is to 
%From the equations of motion 
%\begin{subequations}
%\begin{align}
%  H(\mathbf{p}, \mathbf{q}) = \frac{\mathbf{p}^2}{2m} + V(\mathbf{q}),\\% + theromstat/barostat
%  \frac{\partial\mathbf{p}}{\partial t} = \frac{\partial V(\mathbf{q})}{\partial\mathbf{q}}, \\
%  \frac{\partial\mathbf{q}}{\partial t} = \frac{\partial \mathbf{p}}{m},
%\end{align}
%\end{subequation}
%we obtain 
%a separation of the computation of kinetic and potential energy into separated modules arises.
%Depending on the type of thermodynamic ensemble and subsequently the thermo- or barostat the kinetic energy and its derivative, the differently evaluated.
% TODO I don't know if these are really easy changes
%As the evaluation of the kinetic energy, even considering the correction by thermo- or barostat, is computationally not significant, a lot of method development and specialized software has been focused on making the evaluation of the potenial energy more efficient.

%The potential energy can be further separated into a short- and long-range term
%\begin{equation}
%  V(\mathbf{q}) = V_\textrm{short}(\mathbf{q}) + V_\textrm{long}(\mathbf{q}).
%\end{equation}
%Often MD software separate the calculation of both parts into separate module as $V_\textrm{short}$ is computed in real space and the $V_\textrm{long}(\mathbf{q})$ typically is computed in Fourier space as much is not reused thus not much can be recomputed.
%Therefore both require different types of data structures  and resolved in complicated 
%and therefore the separation into separate modules naturally arises.
%
%take computationally advantage of using a neighborlist using binning (also known as voxel) algorithms\cite{TODO} while the long-range part can be computationally efficient evaluated in Fourier space using partical mesh methods\cite{TODO}.
%Under the assumption of uniformly distributed system the theoretical complexity is $O(N\log N)$ instead of $O(N^2)$ in the implementation of naive scaling.
%Even though a perfect uniform distribution is almost never fulfilled for a system, it is reasonable to assume one is close enough to such a state for a large part of the states during a simulation that the theoretical scaling is approximative true.
%Note that theoretical scaling arguments completely ignore constant overheads due to preprocessing or improvements due to hardware capacities (e.g. cache locallity, hardware capability of parallel processing) that are essential for real applications. 

%MPI pair contributions of forces requiring reorganisation of contributions to consider ghost neighbors correctly.
%One advantage for such a separation is that the implementation of the thermo- and barostats different ensembles, the calculation of the neighborlist can be separated in different parts of the code.
%optimization of the calculation of the neighborlist can be separately
%An advantage of the development of software dedicated for the creation of interatomic potentials separate from the other parts of the MD software.
%The parts that control how the MD work thus do not need be reimplemented and optimizations of the neighborlist with MPI can be reused.
%A disadvantage of the modularity can be seen in example for a long-range potential.
%While usually 

%\subsection{Application LAMMPS Plumed interface for interfacial effects on BaTiO3}

%\section{Modularity in MLIP}
%There are several things we learn from technical analysis details that prevent a modular design as present it exists for traditional machine learning algorithms.
%Narative:
%An essential prevention for modularity is that the computation of the kernel requires a sparse data format hash mappable due to the species polynomial growth
%Secondly, most ML packages do not support gradients in their methodology.
%A recent develompent.

%\subsection{MLIP interface}

\section{Serialization of MLIP}
In last decade, the rapid development of machine learning models has resulted in numerous interfaces for \texttt{LAMMPS}\cite{QUIP}. %TODO more
While the model accuracy is a good indicator for the usability of a model, it is still not reliable enough to be a guarantee that the model will work for MD software as undersampling of the phase space can always be an issue for any kind of model.
One is therefore forced to retrain the model for a dataset covering a larger part of the phase pace or switch to a more accurate model during the analysis of an experiment.
As all ML models show different trade-offs between accuracy, evaluation time, trainings cost and hyperparameter optimization the set of suitable models depend on the system of interest, the dataset size and given computational resources.
As the dataset size changes over the course of analysis different models become more suitable candidates and thus a different model is more suitable for the analysis.
%It has been shown that linear models can be competetive in high data regime. %TODO verify that
The current software infrastructure of ML models for MD software causes a lot of friction when changing the model type as the MD software needs to be recompiled for a different interface.% and a new workflow pipeline has to be constructed.
Even more crucial is the fact that the model development is often conducted in higher-level languages like \texttt{Python} or \texttt{Julia} due to their flexibility.
This however restricts the usage of the model also to MD packages in the same higher-level langue or requires the implementation of a serialization for the model plus interface for an MD package.
This work restricts the usage of a lot of developed ML models in low-level MD packages which are often required to conduct insightful research.
For nearly five years following its initial publication, the widely-used SchNet model\cite{schu+18jcp} lacked an interface with a low-level MD package.
%This however restricts the usage of the model also to higher-level when no serialization exists. 
%Heteregoneous hardware optimizer: OneAPI, Kokkos
Similar problems exists in industry where models trained with different ML packages need to be shipped to devices with different hardware architectures and different software stacks making it hard to reliably provide the same version of the package on each device.
The industry therefore developed an open standard for machine learning models open neural network exchange (ONNX).
This standard can however not be used for MLIPs as they lack the support of the inference for gradients.
%Since the majority of ML models are constructed in higher-level languages, most MD engines are written in a low-level language for performance reasons.
%It is thus needed to export models to a standardized formate that allows to execute models in the low-level languaes
%Low-level language for efficiency, scripting-like for flexibility

%to export models to one software from memory to disk for reproducibility and hardware independency (e.g. JSON).

The Open Knowledgebase of Interatomic Models (OpenKIM)\cite{karls2020openkim} tries to address the problem.
They developed abstract representations of the data and processing directives necessary to perform a molecular simulation thereby unifying the interfaces of several MD packages to one interface, namely the KIM API\cite{elliott2011kim}.
While it reduces the cost of the number interfaces that are needed, they are far from covering comprehensively all relevant MD packages, as packages like GROMACS\cite{hess+08jctc} and C2PK\cite{kuhne2020cp2k} are missing.
Most of the supported MD packages are implemented in higher-level languages for which a custom MLIP interface can be easily implemented.

A solution we target with the software ecosystem developed in our lab is to use TorchScript as it supports the use of gradients and offers a usage of the model in C++ surpassing the high-level language barrier.
It further supports advances model optimization utilities that can be used optimize complex models by kernel fusioning of the operation graph.
Considering all that it seems like a promising candidate to standardize the landscape of MLIPs.

%\section{Foreign-Function interface [optional]}
%Low-level language for efficiency, scripting-like for flexibility
%Libraries that https://www.swig.org/compat.html
%Check out \url{https://docs.rs/rust_swig/latest/rust_swig/#enums}
%However these are not working with TorchScript, also Julia is missing that is becoming essential for scientific softwared development

%\section{Machine learning ecosystems}
%
%\subsection{Hardware kernels}
%\subsection{scikit-learn - close-form inspired infrastructure}
%- transformers, models
%- fit function needs to be specified that optimizes one object
%- concatenation of models through Pipelines allows parameter optimization over different modules however it is every rigid (sample dimension cannot change over the pipelines)
%\subsection{torch - autograd inspired infrastructure}
%Dataloader:
%- concatenation of indicies
%- caching
%Optimizer:
%- concatenation of indicies
%- caching
%While torch summarizes all existing optimizers under one library, Jax approaches this issue with modularity\cite{jaxopt_implicit_diff}
%\subsection{Standardization array-APIs}
%mention Array-API
%https://data-apis.org/array-api/latest/

%\section{MLIP for applications on ferroelectrics}
%\section{Metadynamic framework using MLIP}
%We have seen development of splining algorithms.
%In the chapter we cover a specific application where this supported the work
%
%A ferroelectric is a material that possesses a permanent electric polarization that can be switched under the action of an external electric field.
%Typically, the emergence of the polar phase is accompanied by a structural transition from a high-symmetry paraelectric state down to a broken-symmetry state with a characteristic long-range dipolar ordering [1, 2].
%Early studies on ferroelectrics using ab-initio calcluation [23, 24] have identified discrepancies between the temperatures at which structural transitions occur in the simulations and the temperatures at which these transitions occur in reality.
%These discrepancies are often reduced by introducing artificial corrections to the simulated pressure [25–27].
%This additional pressure brings the simulation volume closer to the values seen in experiments and drives the predicted Curie temperature towards its experimental value.
%However, this correction tells us little about the physical origins for the discrepancies that are observed in simulations.
%There are multiple sources of error that could be the origin for these discrepancies.
%(1) The GGA functional (PBEsol) [28] used in previous work slightly underestimates the equilibrium volume of cubic BaTiO3.
%(2) The determination of the transition temperature by tracking spontaneous fluctuations between the phases limits the system size that can be studied, despite the reduced computational cost of the ML potential.
%The paraelectric-ferroelectric phase transition needs to be therefore studied by using metadynamics simulations.
%
%%work are adjustments to the
%%In the subsequent sections we will discuss the technica
%%My contributions to this work was firstly, to participate in the development of the MLIP package \texttt{librascal} that was used to conduct DFT simulations at higher levels-of-theory to analyse its effect on the temperature.
%%Specifically, the implementation of selective computations of the partial gradients for certain species was implemented by me to speed up the metadynamics simulation.
%%Secondly, the implementation of an interface to the MD software \texttt{LAMMPS} that allowed to study the finite size effects on the Curie temperature.
%
%To conduct metadynamics a bias term based on the spherical expansion coefficients was computed \texttt{PLUMED} using \texttt{librascal}
%%The interface was implemented by Gareth Tribello.
%To consolidate the forces computed with \texttt{LAMMPS} and the bias term computed with \texttt{PLUMED} to run metadynamics the software-package \texttt{i-PI} was used, that implemented a custom protocol to each of these packages to allow communication betwe.
%A schematic of the interwork between the software packages can be seen in Fig.~\ref{fig:ipi-librascal-plumed}.
%\begin{figure}
%    \includegraphics[width=\textwidth]{fig/ipi-librascal-plumed.pdf}
%    \caption{A schematic showing the interwork of the software pieces to run metadynamic simulations to study interfacial effects of $BaTiO_3$.}
%    \label{fig:ipi-librascal-plumed}
%\end{figure}
